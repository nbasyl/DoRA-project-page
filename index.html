<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DoRA">
  <meta property="og:title" content="DoRA"/>
  <meta property="og:description" content="DoRA"/>
  <meta property="og:url" content="https://nbasyl.github.io/DoRA-project-page/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="sliders/illum_car_0.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
  <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->


  <title>DoRA: Weight-Decomposed Low-Rank Adaptation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>


  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DoRA: Weight-Decomposed Low-Rank Adaptation</h1>
              <h2> <font size="+2.5">  </font></h1>
                <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://nbasyl.github.io/" target="_blank">Shih-Yang Liu</a><sup>1,2,*</sup>,</span>
                <span class="author-block">
                  <a href="https://chienyiwang.github.io/" target="_blank">Chien-Yi Wang</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://hongxu-yin.github.io/" target="_blank">Hongxu Yin</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.pmolchanov.com/" target="_blank">Pavlo Molchanov</a><sup>1</sup>,</span>
                    </br>
                      <span class="author-block">
                        <a href="http://vllab.ee.ntu.edu.tw/ycwang.html" target="_blank">Yu-Chiang Frank Wang</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="https://seng.hkust.edu.hk/about/people/faculty/tim-kwang-ting-cheng" target="_blank">Kwang-Ting Cheng</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://minhungchen.netlify.app/" target="_blank">Min-Hung Chen</a><sup>1</sup>
                  </span>
                  </div>


                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>NVIDIA Research<br><sup>2</sup>HKUST</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Work done during internship at NVIDIA Research</small></span>
                  </div>


                  <div class="column has-text-centered">

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->

                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2402.09353" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                  <span class="link-block">
                    <a href="https://github.com/NVlabs/DoRA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4"> -->
        <!-- <video controls autoplay muted loop>
            <source src="static/videos/teaser_vid.mp4" type="video/mp4">
          </video> -->
          <div class="columns is-centered has-text-centered">
            <img src="static/results/dora_llama3.png" width="70%"/>
          </div>
          <!-- <video controls loop>
            <source src="static/images/teaser_vid.mov">
            Your browser does not support the video tag.
          </video> -->
      <!-- </video> -->
      <br>
      <br>
      <h2 class="subtitle has-text-centered">
        DoRA consistently outperforms LoRA on the LLaMA family models for commonsense reasoning tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants
            have gained considerable popularity because of
            avoiding additional inference costs. However,
            there still often exists an accuracy gap between
            these methods and full fine-tuning (FT). In this
            work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the
            findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes
            the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically
            employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both
            the learning capacity and training stability of
            LoRA while avoiding any additional inference
            overhead. DoRA consistently outperforms LoRA
            on fine-tuning LLaMA, LLaVA, and VL-BART
            on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and
            image/video-text understanding.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->

<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Method Overview</h2>
      </div>
      
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <img src="static/images/dora.png" alt="Method Overview"/>
           <br>
           <br>
           <div class="columns is-centered has-text-centered">
             <p>
              An overview of our proposed DoRA, which decomposes
              the pre-trained weight into magnitude and direction components
              for fine-tuning, especially with LoRA to efficiently update the
              direction component.             </p>
           </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Results</h2>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-five-fifths">
          <!-- <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
           -->
           <h3 class="title is-4">Finetuning LLaMA-7B/13B, LLaMA2-7B and LLaMA3-8B for the commonsense reasoning tasks</h3>

               <!-- Illumination Row -->
               <div class="columns is-centered">
                <img src="static/results/commonsense_results.png" width="100%"/>
               </div>

          <br>
          <br>

          <h3 class="title is-4">Instruction tuning LLaMA-7B and LLaMA2-7B with cleaned Alpaca dataset.</h3>
              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="static/results/dvora.png" width="60%"/>
              </div>
          
          <br>
          <br>
          <h3 class="title is-4">Finetuning VL-BART for the image/video-text understanding tasks</h3>

              <!-- Illumination Row -->
              <div class="columns is-centered">
               <img src="static/results/vlbart1.png" width="90%"/>
              </div>
              <div class="columns is-centered">
                <img src="static/results/vlbart2.png" width="90%"/>
               </div>

          <br><br>

          <h3 class="title is-4">Visual instruction tuning (LLaVA-1.5-7B)</h3>

              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="static/results/llava.png" width="90%"/>
              </div>
    

              <h3 class="title is-4">QDoRA vs. QLoRA vs. FT</h3>

              <!-- Illumination Row -->
              <div class="columns is-centered">
                <img src="static/results/Qdora.png" width="100%"/>
              </div>

           <script>
               // JavaScript code will go here
               // Function to update the image source
           function updateImage(sliderId, imageId, baseName) {
               // illumination_strings = ["taxis_", "lions_8_", "car_10_", "tiger_10_", "table_10_"]
               // var sliderValue = 0;
               // console.log(baseName)
               // if(illumination_strings.includes(baseName)){
                 // /sliderValue = 14 + 27 - document.getElementById(sliderId).value;
               // }/
               // else if(baseName=="vacuum_10_"){
                 // sliderValue = 16 + 27 - document.getElementById(sliderId).value;
               // }
               // else{
                 sliderValue = document.getElementById(sliderId).value;
               // }
               // if(baseName.includes(dollyzoom))
               var newImagePath = "sliders/" + baseName + sliderValue + ".png";
               // console.log(newImagePath) // Adjust this path as per your images


               document.getElementById(imageId).src = newImagePath;
           }

           // Attach event listeners to each slider

           function updateMultiConceptImage(baseId, baseName) {
               var sliderValueA = document.getElementById(baseId + "a").value;

               illumination_strings = ["horses", "bear"]
               var sliderValueB = 0;
               // console.log(baseName)
               if(illumination_strings.includes(baseName)){
                 sliderValueB = 14 + 27 - document.getElementById(baseId + "b").value;
               }
               else{
                 sliderValueB = document.getElementById(baseId + "b").value;
               }

               // var sliderValueB = document.getElementById(baseId + "b").value;
               var newImagePath = "supp/" + baseName + "_" + sliderValueA + "_" + sliderValueB + ".png";
               // console.log("image-" + baseId) // Adjust this path as per your images
               document.getElementById("image-" + baseName).src = newImagePath;
           }

           function attachMultiConceptSliderEvents(baseId, baseName) {
               var sliderA = document.getElementById("slider-" + baseId + "a");
               var sliderB = document.getElementById("slider-" + baseId + "b");

               sliderA.addEventListener("input", function() {
                   updateMultiConceptImage("slider-" + baseId, baseName);
               });

               sliderB.addEventListener("input", function() {
                   updateMultiConceptImage("slider-" + baseId, baseName);
               });
           }
           function attachSliderEvents() {
               var sliders = ["illum_car", "illum_hippo", "illum_toaster",
                              "run_fox", "run_squirrel", "wing_owl", "wing_colorbird",
                              "dollychair1", "dollychair2"];

               sliders.forEach(function(slider) {
                   document.getElementById("slider-" + slider).addEventListener("input", function() {
                       updateImage("slider-" + slider, "image-" + slider, slider + "_");
                   });
               });

               // attachMultiConceptSliderEvents("horses", "horses");
               attachMultiConceptSliderEvents("bear", "bear");
               attachMultiConceptSliderEvents("seagull", "seagull");
           }
           document.addEventListener("DOMContentLoaded", attachSliderEvents);

           // Initialize the sliders once the window is loaded
           window.onload = attachSliderEvents;

           </script>


          <!-- </div> -->
        </div>
        <!-- <div class="column is-four-fifths">
          <img src="static/images/results.pdf" alt="3DMiner Overview"/>
        </div> -->
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<div class="container is-max-desktop content">
  <div class="columns is-centered has-text-centered"><h2 class="title">Implementation of DoRA compared to LoRA in PyTorch-like code</h2></div>
  
  <pre><code class="python">
  ## LoRA forward pass
  def forward(self, x: torch.Tensor):
    base_result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
    dropout_x = self.lora_dropout(x)

    result += (self.lora_B(self.lora_A(dropout_x.to(self.lora_A.weight.dtype)))) * self.scaling
    return result

  ## DoRA forward pass
  def forward(self, x: torch.Tensor):
    base_result = F.linear(x, transpose(self.weight, self.fan_in_fan_out))
    dropout_x = self.lora_dropout(x)

    new_weight_v = self.weight + (self.lora_B.weight @ self.lora_A.weight) * self.scaling
    norm_scale = self.weight_m_wdecomp.weight.view(-1) / (torch.linalg.norm(new_weight_v,dim=1)).detach()
    result = base_result + (norm_scale-1) * (F.linear(dropout_x, transpose(self.weight, self.fan_in_fan_out)))
    result += ( norm_scale * (self.lora_B(self.lora_A(dropout_x.to(self.lora_A.weight.dtype))))) * self.scaling
    if not self.bias is None:
      result += self.bias.view(1, -1).expand_as(result)
    return result
  </code></pre>
</div>                
<div class="columns is-centered has-text-centered"><h2 class="title">Quick Start with FSDP/QDoRA from Answer.AI</h2></div>
<br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">QDoRA + Fully Sharded Data Parallel(FSDP) is now supported by Answer.AI </font></div></div>


<div class="columns is-centered has-text-centered"><pre><code>git clone https://github.com/AnswerDotAI/fsdp_qlora</code></pre></div>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">Start finetuning LLMs on consumer GPUs!</font></div></div>

<br>
<div class="columns is-centered has-text-centered"><h2 class="title">Quick Start with Hugging Face PEFT and Diffusers</h2></div>
<br>
<div class="columns is-centered has-text-centered"><h3 class="title">Hugging Face PEFT</h3></div>
<br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">DoRA is now supported by the Huggingface PEFT package. You can install the PEFT package using</font></div></div>
<br>
<div class="columns is-centered has-text-centered"><pre><code>pip install git+https://github.com/huggingface/peft.git -q</code></pre></div>
<br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">After PEFT is installed, you can simply set the use_dora argument of LoRAConfig to True for applying DoRA. An example could be as follows:</font></div></div>
<br>

<div class="container is-max-desktop content">  
  <pre><code class="python">
    from peft import LoraConfig, get_peft_model

    # Initialize DoRA configuration
    config = LoraConfig(
        ...
        use_dora=True
        ...
    )
  </code></pre>
</div>      
<br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">Please refer to the <a href="https://huggingface.co/docs/peft/en/developer_guides/lora#weight-decomposed-low-rank-adaptation-dora" target="_blank">official documentation</a> for more details.</font></div></div>
<br>  
<div class="columns is-centered has-text-centered"><h3 class="title">Hugging Face Diffusers</h3></div>
<br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">You can also toy with DoRA on finetuning Diffusion Model. See <a href="https://github.com/huggingface/diffusers/tree/main/examples/advanced_diffusion_training#dora-training" target="_blank">huggingface/diffusers</a>. Another good tutorial would be this <a href="https://colab.research.google.com/drive/134mt7bCMKtCYyYzETfEGKXT1J6J50ydT?usp=sharing#scrollTo=23d6bb49-3469-4e23-baf5-25b2344b599d" target="_blank">collab notebook</a> from <a href="https://twitter.com/linoy_tsaban" target="_blank">Linoy Tsaban</a>.</font></div></div>
<br>

<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><h3 class="title is-4">Some DoRA v.s LoRA diffusion finetuning results</h3></div></div>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">Example From <a href="https://twitter.com/linoy_tsaban" target="_blank">Linoy Tsaban</a> (Images generated by DoRA are on the left and LoRA on the right):</font></div></div>
<br>  
<!-- Illumination Row -->
<div class="columns is-centered">
  <img src="static/results/dora_lora_yoda_emoji.jpg" width="1000"/>
</div>
<br>  
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">Example From <a href="https://twitter.com/mervenoyann" target="_blank">merve</a></font></div></div>
<br>
<div class="columns is-centered">
  <img src="static/results/dora_lora_lego.jpeg" width="1000"/>
</div>
<br>
<br>
<!-- <br>
<div class="columns is-centered has-text-centered"><div class="container is-max-desktop content"><font size="+1">In general DoRA finetuning on diffusion model is still experimental and is likely to require <strong>different hyperparameter values</strong> to perform best compared to LoRA.
  <br>
  <br>
  Specifically, people have noticed 2 differences to take into account your training:
  <ul>
    <li>LoRA seem to converge faster than DoRA (so a set of parameters that may lead to overfitting when training a LoRA may be working well for a DoRA)</li>
    <li>DoRA quality superior to LoRA especially in lower ranks the difference in quality of DoRA of rank 8 and LoRA of rank 8 appears to be more significant than when training ranks of 32 or 64 for example.
    </li>
  </ul>
  </font></div></div>
<br> -->


<!--BibTex citation -->
<div class="columns is-centered has-text-centered"><h2 class="title">BibTeX</h2></div>
    <div class="container is-max-desktop content">

      <pre><code>@article{liu2024dora,
        title={DoRA: Weight-Decomposed Low-Rank Adaptation},
        author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
        journal={arXiv preprint arXiv:2402.09353},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
